# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GoNHq6a7MBeyvNBGDM6m8IAwummNHyto
"""

import sys

import numpy as np
import sklearn
import pandas as pd
import matplotlib.pyplot as plt
from tabulate import tabulate
from scipy.stats import pearsonr

from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn import linear_model

from google.colab import files
data_file = files.upload()

import io
data = pd.read_csv(io.BytesIO(data_file['SAheart.data.csv']), index_col=0)

data.head()

data["famhist"] = data["famhist"] == "Present"
data["famhist"]=data["famhist"].astype(int)

data.corr()

#features listed in order of correlation with label (highest to lowest)
features_corr = data[['age', 'tobacco', 'ldl', 'adiposity', 'sbp', 'typea', 'obesity', 'alcohol']]
features_corr.head()

# Import data, convert to numpy arrays, and preprocess string ground truth to ints
feature_names = ['Intercept'] + [d for d in data.columns if d != 'chd' and d != 'famhist']

train_data = np.concatenate((np.ones((data.shape[0],1)), data[list(col for col in data.columns if col != 'chd' and col != 'famhist')].to_numpy()), axis=1)
test_data = data['chd'].to_numpy().reshape((len(data),1))

# Split data
x_train, x_test, y_train, y_test= train_test_split(train_data, test_data, test_size=0.2, random_state = 0)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size = 0.5, random_state = 0)

# Normalize train data
def normalize(x, mean, std):
  for i in range(1, x.shape[1]):
    #x[:,i] = (x[:,i] - np.mean(x_train[:,i])) / (np.std(x_train[:,i]) + 1e-5)
    x[:,i] = (x[:,i] - mean[i]) / (std[i] + 1e-5)
  return x

def count_correct_predictions(y_hat, y_test):
  return sum(y_hat.T == y_test)

def sigmoid(z): 
    return 1 / (1 + np.exp(-z))

def mse_error(y, y_hat):
  return mean_squared_error(y_hat, y.T)

def get_minibatch(x, y, batchsize):
  num_batches = x.shape[0] // batchsize

  for i in range(num_batches):
    # draw random numbers from 0 to the number of data
    indx = np.random.randint(0, x.shape[0], batchsize)
    yield (x[indx,:].reshape(batchsize, -1), y[indx,:].reshape(batchsize, 1))

# Normalize train, validation, and test features
x_train_mean = np.zeros(x_train.shape[1])
x_train_std = np.zeros(x_train.shape[1])
for i in range(1, x_train.shape[1]):
    x_train_mean[i] = np.mean(x_train[:,i])
    x_train_std[i] = np.std(x_train[:,i])

x_train = normalize(x_train, x_train_mean, x_train_std)
x_val = normalize(x_val, x_train_mean, x_train_std)
x_test = normalize(x_test, x_train_mean, x_train_std)

"""Figure 4.12"""

import seaborn as sns

sns.pairplot(data[['sbp', 'tobacco', 'ldl', 'famhist', 'obesity', 'alcohol', 'age', 'chd']], hue='chd')

"""**1. Unregularized Model**"""

def initialize_weights(size):
  return np.zeros((1, size)), 0

def model_optimize(w, X, Y):
    # number of training data
    m = X.shape[0]
    
    #Prediction
    final_result = sigmoid(np.matmul(w, X.T))
    
    Y_T = Y.T
    #eqn 4.20
    log_likelihood = np.sum(Y_T*np.log(final_result) + (1-Y_T)*(np.log(1-final_result)))
    #
    
    #Gradient calculation
    dw = (Y - final_result) * X 
 
    grads = {"dw": dw}
    
    return grads, log_likelihood

def model_predict(w, x_train, y_train, learning_rate, no_iterations, batchsize):
    log_likelihoods = []
    for i in range(no_iterations):
        # SGD
        for x_batch, y_batch in get_minibatch(x_train, y_train, batchsize):
          #
          grads, log_likelihood = model_optimize(w, x_batch, y_batch)
          #
          dw = grads["dw"]

          #weight update
          w = w + learning_rate * (dw)
          #
          
        log_likelihoods.append(log_likelihood)
    
    #final parameters
    coeff = {"w": w}
    gradient = {"dw": dw}
    
    return coeff, gradient, log_likelihoods

def predict(final_pred, m):
    y_pred = np.zeros((1,m))
    for i in range(final_pred.shape[1]):
        if final_pred[0][i] > 0.5:
            y_pred[0][i] = 1
        else:
            y_pred[0][i] = 0
    return y_pred

no_iterations = 100
batchsize = 1
learning_rate = 0.00001

w, b = initialize_weights(x_train.shape[1])

# Fit training data
coeff, gradient, log_likelihoods = model_predict(w, x_train, y_train, learning_rate, no_iterations, batchsize)
y_hat = predict(sigmoid(np.dot(coeff['w'], x_test.T)), x_test.shape[0])

# mse = mse_error(y_hat, y_test)
num_correct_plain = count_correct_predictions(y_hat, y_test)[0]
print(f'Number of correct predictions: {num_correct_plain}/{y_test.shape[0]}')
print(f'Percent correct: {num_correct_plain / y_test.shape[0]}')

"""**2. Stepwise Model**"""

#stepwise forward selection function to select best features for prediction
def forward_selection(x_train, y_train, x_val, y_val, learning_rate, no_iterations):
    max_num_correct = 0
    feature_dict = {'alcohol': 7, 'obesity': 6, 'typea': 5, 'sbp': 1, 'adiposity': 4, 'ldl': 3, 'tobacco': 2, 'age': 8}

    selected_features = []
    selected_features_indices = []
    # coeff_dict = {"Feature": [], "Coefficients": []}

    for item in features_corr.columns:
        w = np.zeros((1, len(selected_features)+2))
        print(selected_features)
        
        if item in selected_features:
            continue
        
        print("Trying feature", item)
        key = feature_dict[item]
        
        print("Current feautrues: ", selected_features_indices + [key])
        features = [0] + selected_features_indices + [key]
        print("1: ", w.shape)
        print("2: ", len(features))
        coeff, gradient, log_likelihood = model_predict(w, x_train[:, features], y_train, learning_rate, no_iterations, 1)
        w_opt = coeff['w']

        # Compute number of correct predictions on validation set
        y_hat = predict(sigmoid(np.dot(w_opt, x_val[:, features].T)), x_val.shape[0])
        num_correct = count_correct_predictions(y_hat, y_val)[0] 
        print(num_correct)

        # Store optimal weights
        if num_correct >= max_num_correct:
          max_num_correct = num_correct
          selected_features.append(item)
          selected_features_indices.append(key)
          W = w_opt

          print(w_opt)
    return W, selected_features

W, selected_features = forward_selection(x_train, y_train, x_val, y_val, .0001, 100)

feature_dict = {'alcohol': 7, 'obesity': 6, 'typea': 5, 'sbp': 1, 'adiposity': 4, 'ldl': 3, 'tobacco': 2, 'age': 8}
indices = [0]
for i in selected_features:
  value = feature_dict[i]
  indices.append(value)
x_best = x_test[:, indices] #extract best feature columns 
y_pred = predict(sigmoid(np.dot(W, x_best.T)), x_best.shape[0])

num_correct_stepwise = count_correct_predictions(y_pred, y_test)[0]
print(f"Number of correct predictions: {num_correct_stepwise}/{y_test.shape[0]}")
print(f'Percent correct: {num_correct_stepwise / y_test.shape[0]}')

"""**3. L2 Regularized**"""

def model_optimize_l2norm(w, X, Y, lamb):
    # number of training data
    m = X.shape[0]
    
    #Prediction
    final_result = sigmoid(np.matmul(w, X.T))
    Y_T = Y.T
    # cost = (-1/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))) + lamb * np.matmul(w, w.T))
    log_likelihood = np.sum(Y_T*np.log(final_result) + (1-Y_T)*(np.log(1-final_result))) + (lamb/2) * np.sum(w**2)
    #
    
    #Gradient calculation
    dw = (Y - final_result) * X

    grads = {"dw": dw}
    
    return grads, log_likelihood

def model_predict_l2norm(w, x_train, y_train, x_val, y_val, learning_rate, no_iterations, lambdas, batchsize):
    max_percent_correct = 0
    W = 0
    gradient = {"dw": 0}
    opt_lambda = 0

    # Loop through lambda to find optimal lambda for l2 penalty
    for lamb in lambdas:
      for i in range(no_iterations):
          #
          # SGD
          for x_batch, y_batch in get_minibatch(x_train, y_train, batchsize):
            grads, log_likelihood = model_optimize_l2norm(w, x_batch, y_batch, lamb)
            #
            dw = grads["dw"]

            # gradient ascent
            w = w + (learning_rate * (dw)) - lamb*w
            #
          
          # select lambda that gives the highest percent correct
          y_hat = predict(sigmoid(np.dot(w, x_val.T)), x_val.shape[0])
          num_correct = count_correct_predictions(y_hat, y_val)[0] 
        
          if num_correct > max_percent_correct:
            #final parameters
            W = w
            gradient = {"dw": dw}
            opt_lambda = lamb
            max_percent_correct = num_correct

    return W, gradient, opt_lambda

def predict(final_pred, m):
    y_pred = np.zeros((1,m))
    for i in range(final_pred.shape[1]):
        if final_pred[0][i] > 0.5:
            y_pred[0][i] = 1
        else:
            y_pred[0][i] = 0
    return y_pred

# Set hyperparameters for logistic regression with L2Norm
no_iterations = 100
learning_rate = 0.0001
batchsize = 1
lambdas = [1e-5, 1e-4, 0.001, 0.01, 0.1, 1]
# lambdas = [1e-4, 1e-3]

# Initialize weight and bias to zeros.
# Initialize using other distribution might help.
w, b = initialize_weights(x_train.shape[1])

# Fit training data
W, gradient, min_lambda = model_predict_l2norm(w, x_train, y_train, x_val, y_val, learning_rate, no_iterations, lambdas, batchsize)

y_hat = predict(sigmoid(np.dot(W, x_test.T)), x_test.shape[0])

# Report result 
num_correct_l2 = count_correct_predictions(y_hat, y_test)[0]
print(f'Number of correct predictions: {num_correct_l2}/{y_test.shape[0]}')
print(f'Percent correct: {num_correct_l2 / y_test.shape[0]}')

# Report results in table
from tabulate import tabulate
print("Result of classifying SA Heart data")
table = [['Model', '% Correct (%)'], 
         ['Plain', num_correct_plain / y_test.shape[0]],
         ['L2 Reg', num_correct_l2 / y_test.shape[0]],
         ['Stepwise', num_correct_stepwise / y_test.shape[0]]]

print(tabulate(table, headers='firstrow'))

"""**Using Wisconsin Breast Cancer Data**"""

data2 = files.upload()

data2_pd = pd.read_csv(io.BytesIO(data2['data.csv']))

data2_pd = pd.read_csv('/content/data.csv', header = 0)

data2_pd.head()

#convert diagnosis to binary
data2_pd.diagnosis[data2_pd.diagnosis == 'M'] = 0
data2_pd.diagnosis[data2_pd.diagnosis == 'B'] = 1

data2_pd = data2_pd.drop(['Unnamed: 32', 'id'], axis=1)
data2_pd.head()

#change diagnosis column to int64 datatype
data2_pd['diagnosis'] = data2_pd['diagnosis'].apply(pd.to_numeric)
# print(data2_pd.dtypes)

#get correlation between features and target
corr_dict = {}
keys = data2_pd.columns

for k in keys:
  if k == 'diagnosis':
    continue
  corr = data2_pd[k].corr(data2_pd['diagnosis'])
  corr_dict[k] = corr
print(corr_dict)

#order from highest to lowest correlation
corr_dict = dict(sorted(corr_dict.items(), key=lambda item: item[1], reverse = True))
# print(corr_dict)

data2_train = data2_pd[list(col for col in data2_pd.columns if col != "Unnamed: 32" and col != 'diagnosis' and col != 'id')]
data2_train.dropna()
data2_train = data2_train.to_numpy()

data2_test = data2_pd['diagnosis'].to_numpy()
data2_test = data2_test.reshape(len(data2_test), 1)

# Split data
x_train2, x_test2, y_train2, y_test2 = train_test_split(data2_train, data2_test, test_size=0.2, random_state = 0)
x_val2, x_test2, y_val2, y_test2 = train_test_split(x_test2, y_test2, test_size = 0.5, random_state = 0)

# Normalize train, validation, and test features
x_train2_mean = np.zeros(x_train2.shape[1])
x_train2_std = np.zeros(x_train2.shape[1])
for i in range(1, x_train2.shape[1]):
    x_train2_mean[i] = np.mean(x_train2[:,i])
    x_train2_std[i] = np.std(x_train2[:,i])

x_train2 = normalize(x_train2, x_train2_mean, x_train2_std)
x_val2 = normalize(x_val2, x_train2_mean, x_train2_std)
x_test2 = normalize(x_test2, x_train2_mean, x_train2_std)

"""**1. Unregularized**"""

no_iterations = 100
batchsize = 1
learning_rate = 0.00001

w, b = initialize_weights(x_train2.shape[1])

# Fit training data
coeff, gradient, log_likelihoods = model_predict(w, x_train2, y_train2, learning_rate, no_iterations, batchsize)
y_hat = predict(sigmoid(np.dot(coeff['w'], x_test2.T)), x_test2.shape[0])

# mse = mse_error(y_hat, y_test)
num_correct_plain2 = count_correct_predictions(y_hat, y_test2)[0]
print(f'Number of correct predictions: {num_correct_plain2}/{y_test2.shape[0]}')
print(f'Percent correct: {num_correct_plain2 / y_test2.shape[0]}')

"""**2. Stepwise Model**"""

indices = {c: i for i, c in enumerate(data2_pd.columns)}
del indices["diagnosis"]
# print(indices)

for i in indices.keys():
  value = indices[i] - 1
  indices[i] = value

# print(indices)

def forward_selection2(x_train, y_train, x_val, y_val, learning_rate, no_iterations):
    n_features = x_train2.shape[1]
    max_num_correct = 0
    
    selected_features = []
    selected_features_indices = []

    for item in corr_dict.keys():
        w = np.zeros((1, len(selected_features)+2))
        if item in selected_features:
            continue
        print("Trying feature", item)
        value = indices[item]
        features = [0] + selected_features_indices + [value]
     
        coeff, gradient, cost = model_predict(w, x_train[:, features], y_train, learning_rate, no_iterations, 1)
        w_opt = coeff['w']

        # Compute number of correct predictions on validation set
        y_hat = predict(sigmoid(np.dot(w_opt, x_val[:, features].T)), x_val.shape[0])
        num_correct = count_correct_predictions(y_hat, y_val)[0] 
      
        if num_correct > max_num_correct:
          max_num_correct = num_correct
          selected_features.append(item)
          selected_features_indices.append(value)
          W = w_opt

    return W, selected_features, selected_features_indices

W, selected_features, selected_features_indices = forward_selection2(x_train2, y_train2, x_val2, y_val2, .001, 100)

indices_best = [0] + selected_features_indices

x_best = x_test2[:, indices_best] #extract best feature columns 
y_pred = predict(sigmoid(np.dot(W, x_best.T)), x_best.shape[0])

# Report result
num_correct_stepwise2 = count_correct_predictions(y_pred, y_test2)[0]
print(f'Number of correct predictions:",  {count_correct_predictions(y_pred, y_test2)[0]}/{y_test2.shape[0]}')
print(f'Percent correct predictions:,  {count_correct_predictions(y_pred, y_test2)[0]/y_test2.shape[0]}')

"""**3. L2 Regularized**"""

# Set hyperparameters for logistic regression with L2Norm
no_iterations = 100
learning_rate = 0.001
lamb = 0.001
batchsize = 1
lambdas = [1e-5, 1e-4, 0.001, 0.01, 0.1, 1]

# Initialize weight and bias to zeros.
# Initialize using other distribution might help.
w, b = initialize_weights(x_train2.shape[1])

# Fit training data
W, gradient, min_lambda = model_predict_l2norm(w, x_train2, y_train2, x_val2, y_val2, learning_rate, no_iterations, lambdas, batchsize)

y_hat2 = predict(sigmoid(np.dot(W, x_test2.T)), x_test2.shape[0])

num_correct_l22 = count_correct_predictions(y_hat2, y_test2)[0]
print(f'Number of correct predictions: {num_correct_l22}/{y_test2.shape[0]}')
print(f'Number of correct predictions: {num_correct_l22 / y_test2.shape[0]}')

# Report results in table
from tabulate import tabulate
print("Resuls of classifying breast cancer data")
table = [['Model', '% Correct (%)'], 
         ['Plain', num_correct_plain2 / y_test2.shape[0]],
         ['L2 Reg', num_correct_l22 / y_test2.shape[0]],
         ['Stepwise', num_correct_stepwise2 / y_test2.shape[0]]]

print(tabulate(table, headers='firstrow'))

"""**Stretch Goal 1: Implement L1 Regularization**"""

indices = np.random.permutation(len(x_train))

'''
Reference :
Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty
Yoshimasa Tsuruoka, Junichi Tsujii, Sophia Ananiadou†
'''

def train_l1_reg(x, y, x_val, y_val, iterations, lambdas, learning_rate):
  min_lambda = -1
  max_num_correct = 0
  num_features = x.shape[1]
  num_data = x.shape[0]
  W = np.zeros((1, x.shape[1]))

  # Initialize q (L1 penalty actually received)
  u = 0
  # q = np.zeros((1, num_features))[0]
  q = 0
  for lamb in lambdas:
    w = np.array([1e-5] * x.shape[1])
    for i in range(iterations):
      # total L1 penalty could have received until interation i 
      u = u + learning_rate * lamb/x.shape[0]

      x_single = x[indices[i], :]
      y_single = y[indices[i], :]
      w, q = update_weights(x_single, y_single, w, u, q)

    # Compute number of correct predictions on validation set
    y_hat = predict(sigmoid(np.dot(w.reshape(1, len(w)), x_val.T)), x_val.shape[0])
    num_correct = count_correct_predictions(y_hat, y_val)[0] 
    #print("num correct: ", num_correct)
      
    if num_correct >= max_num_correct:
      W = w
      max_num_correct = num_correct
      min_lambda = lamb

  return W, min_lambda

def update_weights(x, y, w, u, q):
  for i in range(len(w)):
    if w[i] == 0:
      continue

    else:
      # Update weight using gradient descent
      dw = (y - np.dot(w, x.T)) * x[i] 
      w[i] = w[i] + learning_rate * dw

      temp = w[i]

      # Implement SGD-L1 (Cumulative)
      w[i] = max(0, w[i] - (u + q)) + min(0, w[i] + (u - q))

      q = q + (w[i] - temp)
    
  return w, q

# Initialize hyperparameters
iterations = 200
lambdas = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1] 
# lambdas = [0.001]
learning_rate = 0.001

W, min_lambda = train_l1_reg(x_train, y_train, x_val, y_val, iterations, lambdas, learning_rate)

print(feature_names)
feature_coeffs = {}
for i in range(len(W)):
  feature_coeffs[feature_names[i]] = W[i]

print(dict(sorted(feature_coeffs.items(), key=lambda item: item[1], reverse=True)))

y_hat_bin = predict(sigmoid(np.dot(W.reshape(1,len(W)), x_test.T)), x_test.shape[0])
print(f'Number of correct predictions: {count_correct_predictions(y_hat_bin, y_test)[0]}/{y_test.shape[0]}')
print(f'Percent correct predictions: {count_correct_predictions(y_hat_bin, y_test)[0] / y_test.shape[0]}')

"""The top three significant features collected from L1 SGD were sbp, tobacco, and ldl. In stepwise, tobacco and age showed to be the most significant. The two models both showed that tobacco is an important factor in determining heart disease. L1 didn't select ldl to be important but this can be due to small data and the fact that we are using correlation to order significance in stepwise. However, the two models resulted in reasonable performance and their results generally agree.

**Stretch Goal 2: Multinomial Regression**
"""

from google.colab import files
data_file = files.upload()

header=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
data = pd.read_csv(io.BytesIO(data_file['iris.data']), names=header)

data.head()

data.describe()

data.corr()

"""Figure 4.12"""

import seaborn as sns

sns.pairplot(data[['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']], hue='class')

from tensorflow.keras.utils import to_categorical

data = data.replace(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], [0, 1, 2])

input = np.concatenate((np.ones((data.shape[0],1)), data[list(col for col in data.columns if col != 'class')].to_numpy()), axis=1)
output =  data['class'].to_numpy().reshape((len(data),1))

def one_hot(output):
  one_hot_output = to_categorical(output, num_classes=3)
  return one_hot_output

X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.2, random_state=0)

# Normalize train data
def normalize(x, mean, std):
  for i in range(1, x.shape[1]):
    x[:,i] = (x[:,i] - mean[i]) / (std[i] + 1e-5)
  return x

# Normalize train, validation, and test features
X_train_mean = np.zeros(X_train.shape[1])
X_train_std = np.zeros(X_train.shape[1])
for i in range(1, X_train.shape[1]):
    X_train_mean[i] = np.mean(X_train[:,i])
    X_train_std[i] = np.std(X_train[:,i])

X_train = normalize(X_train, X_train_mean, X_train_std)
#X_val = normalize(X_val, X_train_mean, X_train_std)
X_test = normalize(X_test, X_train_mean, X_train_std)

actual_output = y_test
y_train = one_hot(y_train)
y_test = one_hot(y_test)

no_iterations = 1000000
learning_rate = 0.00001

w = np.zeros((3, X_train.shape[1]))

def multi_model_optimize(w, X, Y):
    # number of training data
    m = X.shape[0]
    
    #Prediction
    final_result = sigmoid(np.matmul(w, X.T))
    """
    print("final result shape: ", np.shape(final_result))
    print("Y.T shape: ", np.shape(Y.T))
    print("X shape: ", np.shape(X))
    """
    Y_T = Y.T
    #eqn 4.20
    log_likelihood = np.sum(Y_T*np.log(final_result) + (1-Y_T)*(np.log(1-final_result)))
    #
    
    #Gradient calculation
    dw = (Y.T - final_result) @ X 
 
    grads = {"dw": dw}
    
    return grads, log_likelihood

def multi_model_predict(w, x_train, y_train, learning_rate, no_iterations):
    log_likelihoods = []
    for i in range(no_iterations):
        # SGD
        
        grads, log_likelihood = multi_model_optimize(w, x_train, y_train)
        #
        dw = grads["dw"]

        #weight update
        w = w + learning_rate * (dw)
        #
          
        log_likelihoods.append(log_likelihood)
    
    #final parameters
    coeff = {"w": w}
    gradient = {"dw": dw}
    
    return coeff, gradient, log_likelihoods

def multi_predict(final_pred, m):
    y_pred = np.zeros(np.shape(final_pred))
    for i in range(final_pred.shape[1]):
      y_pred[np.where(final_pred==np.max(final_pred[:,i]))] = 1
    return y_pred.T

# Fit training data
coeff, gradient, log_likelihoods = multi_model_predict(w, X_train, y_train, learning_rate, no_iterations)
y_hat = multi_predict(sigmoid(np.dot(coeff['w'], X_test.T)), X_test.shape[0])

def multi_accuracy(actual, predicted):
  header = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
  print(f'Number of total correct predictions of: {sum(actual == predicted)}/{actual.shape[0]}')
  print(f'Percent correct: {sum(actual == predicted) / actual.shape[0]}%')
  for i in [0,1,2]:
    index = np.where(actual == i)
    num_correct = sum(actual[index] == predicted[index])
    total = actual[index].shape[0]
    print(f'Number of correct predictions of {header[i]}: {num_correct}/{total}')
    print(f'Percent correct: {num_correct / total}')

multi_accuracy(actual_output.reshape((np.shape(np.argmax(y_hat, axis=1)))), np.argmax(y_hat, axis=1))
